#!/usr/bin/env python3
"""
üöÄ COMPLETE STANDALONE RBI BATCH BACKTESTER - FULLY WORKING VERSION
Moon-Dev Pattern - Ready to Run

This script:
1. Fetches market data (candles) from HTX API
2. Reads JSON strategy files from strategy_library/
3. Generates Python backtest code with DeepSeek
4. Auto-debugs the code (10 iterations)
5. Executes backtests using backtesting.py
6. Optimizes for 50%+ returns (10 iterations)
7. Tests across multiple configurations
8. Gets LLM swarm consensus (DeepSeek + GPT-4o + Claude)
9. Saves approved strategies to batch_backtests/approved/

Usage:
    pip install openai anthropic backtesting pandas numpy talib requests
    export DEEPSEEK_API_KEY="your_key"
    export OPENAI_API_KEY="your_key"
    export ANTHROPIC_API_KEY="your_key"
    python rbi_complete.py
"""

import json
import logging
import os
import sys
import time
import traceback
import subprocess
import tempfile
import re
import requests
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

try:
    import pandas as pd
    import numpy as np
except ImportError:
    print("‚ùå Missing dependencies!")
    print("Run: pip install openai anthropic backtesting pandas numpy talib requests")
    sys.exit(1)

# =========================================================================================
# CONFIGURATION
# =========================================================================================

class Config:
    """Configuration"""
    
    # API Keys
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")
    
    # Paths
    PROJECT_ROOT = Path.cwd()
    STRATEGY_LIBRARY_DIR = PROJECT_ROOT / "strategy_library"
    BATCH_BACKTEST_DIR = PROJECT_ROOT / "batch_backtests"
    DATE_DIR = BATCH_BACKTEST_DIR / datetime.now().strftime("%Y%m%d")
    BACKTEST_CODE_DIR = DATE_DIR / "backtests"
    APPROVED_DIR = BATCH_BACKTEST_DIR / "approved"
    DATA_DIR = PROJECT_ROOT / "data"
    
    # Market Data
    BTC_DATA_PATH = DATA_DIR / "BTC-USD-15m.csv"
    ETH_DATA_PATH = DATA_DIR / "ETH-USD-15m.csv"
    SOL_DATA_PATH = DATA_DIR / "SOL-USD-15m.csv"
    
    # RBI Settings
    TARGET_RETURN_PERCENT = 50.0
    MAX_DEBUG_ITERATIONS = 10
    MAX_OPTIMIZATION_ITERATIONS = 10
    BACKTEST_TIMEOUT_SECONDS = 60
    
    # Moon-Dev Standards
    MIN_WIN_RATE = 0.55
    MIN_PROFIT_FACTOR = 1.5
    MIN_SHARPE_RATIO = 1.0
    MAX_DRAWDOWN = 0.20
    MIN_TRADES = 50
    CONSENSUS_REQUIRED_VOTES = 2

# Create directories
Config.BATCH_BACKTEST_DIR.mkdir(parents=True, exist_ok=True)
Config.DATE_DIR.mkdir(parents=True, exist_ok=True)
Config.BACKTEST_CODE_DIR.mkdir(parents=True, exist_ok=True)
Config.APPROVED_DIR.mkdir(parents=True, exist_ok=True)
Config.DATA_DIR.mkdir(parents=True, exist_ok=True)

# =========================================================================================
# LOGGING
# =========================================================================================

def setup_logging():
    """Setup logging"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = Config.BATCH_BACKTEST_DIR / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    log_file = log_dir / f"rbi_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%H:%M:%S',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger("RBI")

# =========================================================================================
# MARKET DATA FETCHER
# =========================================================================================

class HTXDataFetcher:
    """Fetch market data from HTX API"""
    
    BASE_URL = "https://api.huobi.pro"
    
    @classmethod
    def fetch_candles(cls, symbol: str, interval: str = "15min", limit: int = 2000) -> Optional[pd.DataFrame]:
        """
        Fetch candles from HTX
        
        Args:
            symbol: Trading pair (e.g., 'btcusdt')
            interval: Timeframe ('1min', '5min', '15min', '60min', '4hour', '1day')
            limit: Number of candles
        """
        try:
            url = f"{cls.BASE_URL}/market/history/kline"
            params = {
                "symbol": symbol.lower(),
                "period": interval,
                "size": limit
            }
            
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            if data.get("status") != "ok":
                return None
            
            df = pd.DataFrame(data["data"])
            df.columns = ["id", "Open", "Close", "Low", "High", "Amount", "Vol", "Count"]
            df["Datetime"] = pd.to_datetime(df["id"], unit='s')
            df = df[["Datetime", "Open", "High", "Low", "Close", "Vol"]]
            df.columns = ["Datetime", "Open", "High", "Low", "Close", "Volume"]
            df = df.sort_values("Datetime").reset_index(drop=True)
            
            return df
            
        except Exception as e:
            print(f"‚ùå Error fetching {symbol}: {e}")
            return None
    
    @classmethod
    def ensure_data_files(cls):
        """Ensure all data files exist"""
        assets = [
            ("btcusdt", Config.BTC_DATA_PATH),
            ("ethusdt", Config.ETH_DATA_PATH),
            ("solusdt", Config.SOL_DATA_PATH)
        ]
        
        for symbol, path in assets:
            if path.exists():
                # Check if data is recent (< 24h old)
                mod_time = datetime.fromtimestamp(path.stat().st_mtime)
                if datetime.now() - mod_time < timedelta(hours=24):
                    print(f"‚úÖ Using existing: {path.name}")
                    continue
            
            print(f"üìä Fetching {symbol.upper()}...")
            df = cls.fetch_candles(symbol)
            
            if df is not None:
                df.to_csv(path, index=False)
                print(f"‚úÖ Saved {len(df)} candles to {path.name}")
            else:
                print(f"‚ùå Failed to fetch {symbol}")

# =========================================================================================
# MODEL FACTORY
# =========================================================================================

class ModelFactory:
    """Simple LLM interface"""
    
    @staticmethod
    def call_llm(model_type: str, model_name: str, prompt: str, system_prompt: str = "", 
                 temperature: float = 0.7, max_tokens: int = 4000) -> str:
        """Call LLM"""
        try:
            if model_type == "deepseek" and Config.DEEPSEEK_API_KEY:
                from openai import OpenAI
                client = OpenAI(api_key=Config.DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                response = client.chat.completions.create(
                    model=model_name, messages=messages,
                    temperature=temperature, max_tokens=max_tokens
                )
                return response.choices[0].message.content
                
            elif model_type == "openai" and Config.OPENAI_API_KEY:
                from openai import OpenAI
                client = OpenAI(api_key=Config.OPENAI_API_KEY)
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                response = client.chat.completions.create(
                    model=model_name, messages=messages,
                    temperature=temperature, max_tokens=max_tokens
                )
                return response.choices[0].message.content
                
            elif model_type == "anthropic" and Config.ANTHROPIC_API_KEY:
                from anthropic import Anthropic
                client = Anthropic(api_key=Config.ANTHROPIC_API_KEY)
                response = client.messages.create(
                    model=model_name, max_tokens=max_tokens,
                    temperature=temperature,
                    system=system_prompt if system_prompt else "You are a helpful assistant.",
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text
            else:
                raise Exception(f"No API key for {model_type}")
        except Exception as e:
            raise Exception(f"LLM call failed for {model_type}/{model_name}: {e}")

# =========================================================================================
# RBI ENGINE
# =========================================================================================

class RBIEngine:
    """Complete RBI (Research-Backtest-Implement) Engine"""
    
    def __init__(self, logger):
        self.logger = logger
    
    def process_strategy(self, strategy: Dict) -> Tuple[bool, Optional[str]]:
        """
        Process single strategy through complete RBI cycle
        
        Returns:
            (approved, reason)
        """
        strategy_name = strategy.get("name", "Unknown")
        
        try:
            # PHASE 1: Research
            self.logger.info("üìö Phase 1: Research")
            research = self._research_strategy(strategy)
            
            # PHASE 2: Generate code
            self.logger.info("ü§ñ Phase 2: Code Generation")
            code = self._generate_backtest_code(strategy, research)
            if not code:
                return False, "Code generation failed"
            
            # PHASE 3: Auto-debug
            self.logger.info("üîß Phase 3: Auto-Debug Loop")
            executable_code = self._auto_debug_loop(code, strategy)
            if not executable_code:
                return False, "Auto-debug failed"
            
            # PHASE 4: Execute
            self.logger.info("‚ö° Phase 4: Execute Backtest")
            results = self._execute_backtest(executable_code, strategy)
            if not results:
                return False, "Execution failed"
            
            self.logger.info(f"üìä Results: Return {results['return_pct']:.1f}%, "
                           f"Sharpe {results['sharpe']:.2f}, Trades {results['trades']}")
            
            # PHASE 5: Optimization (if needed)
            if results['return_pct'] < Config.TARGET_RETURN_PERCENT:
                self.logger.info(f"üîÑ Phase 5: Optimization (target {Config.TARGET_RETURN_PERCENT}%)")
                opt_code, opt_results = self._optimization_loop(executable_code, strategy, results)
                if opt_results and opt_results['return_pct'] > results['return_pct']:
                    executable_code = opt_code
                    results = opt_results
                    self.logger.info(f"‚úÖ Optimized to {results['return_pct']:.1f}%")
            
            # PHASE 6: Multi-config testing
            self.logger.info("üìä Phase 6: Multi-Configuration Testing")
            config_results = self._multi_config_testing(executable_code, strategy)
            
            # PHASE 7: Swarm consensus
            self.logger.info("ü§ù Phase 7: LLM Swarm Consensus")
            approved, votes, best_config = self._llm_swarm_consensus(config_results, strategy, results)
            
            if approved:
                # Save approved strategy
                self._save_approved_strategy(strategy, executable_code, results, votes, best_config)
                return True, "Approved by swarm consensus"
            else:
                return False, f"Rejected by swarm (votes: {votes})"
                
        except Exception as e:
            self.logger.error(f"‚ùå Error: {e}")
            self.logger.error(traceback.format_exc())
            return False, str(e)
    
    def _research_strategy(self, strategy: Dict) -> Dict:
        """Phase 1: Research"""
        prompt = f"""Analyze this trading strategy:

Name: {strategy.get('name', '')}
Description: {strategy.get('description', '')}
Entry: {strategy.get('entry_rules', '')}
Exit: {strategy.get('exit_rules', '')}
Indicators: {strategy.get('indicators', [])}

Provide brief implementation guidance."""
        
        try:
            response = ModelFactory.call_llm(
                "deepseek", "deepseek-chat",
                prompt, temperature=0.3, max_tokens=2000
            )
            return {"analysis": response}
        except:
            return {"analysis": "Research unavailable"}
    
    def _generate_backtest_code(self, strategy: Dict, research: Dict) -> Optional[str]:
        """Phase 2: Generate backtest code"""
        system_prompt = """You are an expert in backtesting.py library.
Generate COMPLETE, EXECUTABLE Python code.

Requirements:
- Use backtesting.py library
- Include ALL imports
- Define Strategy class with init() and next()
- Use self.I() for indicators
- Calculate position sizing
- Print results clearly
- Return ONLY Python code"""

        user_prompt = f"""Generate complete backtest code for:

Strategy: {strategy.get('name', '')}
Description: {strategy.get('description', '')}
Entry: {strategy.get('entry_rules', '')}
Exit: {strategy.get('exit_rules', '')}
Stop Loss: {strategy.get('stop_loss', '')}
Indicators: {strategy.get('indicators', [])}

Data file: {Config.BTC_DATA_PATH}

Return ONLY Python code."""

        try:
            response = ModelFactory.call_llm(
                "deepseek", "deepseek-coder",
                user_prompt, system_prompt,
                temperature=0.3, max_tokens=4000
            )
            
            code = self._clean_code(response)
            
            # Save code
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{timestamp}_{strategy.get('name', 'strategy').replace(' ', '_')}.py"
            filepath = Config.BACKTEST_CODE_DIR / filename
            with open(filepath, 'w') as f:
                f.write(code)
            
            self.logger.info(f"üíæ Code saved: {filename}")
            return code
            
        except Exception as e:
            self.logger.error(f"Code generation failed: {e}")
            return None
    
    def _clean_code(self, response: str) -> str:
        """Remove markdown from LLM response"""
        if "```python" in response:
            response = response.split("```python")[1].split("```")[0]
        elif "```" in response:
            response = response.split("```")[1].split("```")[0]
        return response.strip()
    
    def _auto_debug_loop(self, code: str, strategy: Dict) -> Optional[str]:
        """Phase 3: Auto-debug loop"""
        for iteration in range(1, Config.MAX_DEBUG_ITERATIONS + 1):
            self.logger.info(f"   Iteration {iteration}/{Config.MAX_DEBUG_ITERATIONS}")
            
            # Test syntax
            try:
                compile(code, '<string>', 'exec')
                self.logger.info("   ‚úÖ Syntax valid")
                return code  # Success!
            except SyntaxError as e:
                self.logger.warning(f"   ‚ùå Syntax error: {e}")
                code = self._fix_code_with_llm(code, str(e), strategy)
        
        return None
    
    def _fix_code_with_llm(self, code: str, error: str, strategy: Dict) -> str:
        """Fix code with LLM"""
        prompt = f"""Fix this code:

```python
{code}
```

Error: {error}

Strategy: {strategy.get('name', '')}

Return COMPLETE fixed code."""

        try:
            response = ModelFactory.call_llm(
                "deepseek", "deepseek-coder",
                prompt,
                "Fix Python code. Return ONLY code.",
                temperature=0.2, max_tokens=4000
            )
            return self._clean_code(response)
        except:
            return code
    
    def _execute_backtest(self, code: str, strategy: Dict) -> Optional[Dict]:
        """Phase 4: Execute backtest"""
        try:
            # Create temp file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(code)
                temp_path = f.name
            
            # Execute
            result = subprocess.run(
                [sys.executable, temp_path],
                capture_output=True,
                text=True,
                timeout=Config.BACKTEST_TIMEOUT_SECONDS
            )
            
            # Cleanup
            try:
                os.unlink(temp_path)
            except:
                pass
            
            # Parse output
            output = result.stdout + result.stderr
            metrics = self._parse_output(output)
            
            return metrics
            
        except subprocess.TimeoutExpired:
            self.logger.error("‚ùå Timeout")
            return None
        except Exception as e:
            self.logger.error(f"‚ùå Execution error: {e}")
            return None
    
    def _parse_output(self, output: str) -> Optional[Dict]:
        """Parse backtest output - NO SYNTHETIC DATA"""
        metrics = {
            'return_pct': 0.0,
            'sharpe': 0.0,
            'trades': 0,
            'win_rate': 0.0,
            'profit_factor': 0.0,
            'max_drawdown': 0.0
        }
        
        # Log the raw output for debugging
        self.logger.debug(f"Raw backtest output:\n{output}")
        
        # Try to extract from output using multiple patterns
        try:
            # Return percentage - try multiple formats
            return_patterns = [
                r'Return\s*[:\[]*\s*([+-]?[0-9]+\.?[0-9]*)\s*%',  # "Return: 45.2%"
                r'Return.*?([+-]?[0-9]+\.?[0-9]*)\s*%',  # "Return [Annualized]: 45.2%"
                r'Total Return.*?([+-]?[0-9]+\.?[0-9]*)\s*%',  # "Total Return: 45.2%"
            ]
            for pattern in return_patterns:
                match = re.search(pattern, output, re.IGNORECASE)
                if match:
                    metrics['return_pct'] = float(match.group(1))
                    break
            
            # Sharpe ratio
            sharpe_patterns = [
                r'Sharpe\s+Ratio\s*[:\[]*\s*([+-]?[0-9]+\.?[0-9]*)',
                r'Sharpe.*?([+-]?[0-9]+\.?[0-9]*)',
            ]
            for pattern in sharpe_patterns:
                match = re.search(pattern, output, re.IGNORECASE)
                if match:
                    metrics['sharpe'] = float(match.group(1))
                    break
            
            # Number of trades
            trades_patterns = [
                r'#\s*Trades\s*[:\[]*\s*([0-9]+)',
                r'(?:Trades|Number of Trades).*?([0-9]+)',
            ]
            for pattern in trades_patterns:
                match = re.search(pattern, output, re.IGNORECASE)
                if match:
                    metrics['trades'] = int(match.group(1))
                    break
            
            # Win rate
            win_patterns = [
                r'Win\s+Rate\s*[:\[]*\s*([0-9]+\.?[0-9]*)\s*%',
                r'Win Rate.*?([0-9]+\.?[0-9]*)\s*%',
            ]
            for pattern in win_patterns:
                match = re.search(pattern, output, re.IGNORECASE)
                if match:
                    rate = float(match.group(1))
                    metrics['win_rate'] = rate / 100 if rate > 1 else rate
                    break
            
            # Profit factor
            pf_patterns = [
                r'Profit\s+Factor\s*[:\[]*\s*([0-9]+\.?[0-9]*)',
                r'Profit Factor.*?([0-9]+\.?[0-9]*)',
            ]
            for pattern in pf_patterns:
                match = re.search(pattern, output, re.IGNORECASE)
                if match:
                    metrics['profit_factor'] = float(match.group(1))
                    break
            
            # Max drawdown
            dd_patterns = [
                r'Max\.?\s+Drawdown\s*[:\[]*\s*[-]?([0-9]+\.?[0-9]*)\s*%',
                r'Max Drawdown.*?[-]?([0-9]+\.?[0-9]*)\s*%',
            ]
            for pattern in dd_patterns:
                match = re.search(pattern, output, re.IGNORECASE)
                if match:
                    dd = float(match.group(1))
                    metrics['max_drawdown'] = dd / 100 if dd > 1 else dd
                    break
                    
        except Exception as e:
            self.logger.error(f"‚ùå Error parsing metrics: {e}")
            self.logger.error(f"Output was: {output[:500]}")
            return None
        
        # CRITICAL: If parsing failed, return None (strategy fails)
        # NO SYNTHETIC DATA - this wastes premium API calls
        if metrics['return_pct'] == 0.0 and metrics['trades'] == 0:
            self.logger.error("‚ùå FAILED to parse backtest output - NO METRICS FOUND")
            self.logger.error(f"Output sample: {output[:1000]}")
            return None
        
        self.logger.info(f"‚úÖ Parsed metrics: Return {metrics['return_pct']:.1f}%, Sharpe {metrics['sharpe']:.2f}, Trades {metrics['trades']}")
        return metrics
    
    def _optimization_loop(self, code: str, strategy: Dict, results: Dict) -> Tuple[str, Dict]:
        """Phase 5: Optimization loop"""
        best_code = code
        best_results = results
        best_return = results.get('return_pct', 0)
        
        for iteration in range(1, Config.MAX_OPTIMIZATION_ITERATIONS + 1):
            self.logger.info(f"   Optimization {iteration}/{Config.MAX_OPTIMIZATION_ITERATIONS}")
            self.logger.info(f"   Current: {best_return:.1f}% (target: {Config.TARGET_RETURN_PERCENT}%)")
            
            # Optimize with LLM
            opt_code = self._optimize_with_llm(best_code, best_results, strategy)
            if not opt_code:
                continue
            
            # Test
            opt_results = self._execute_backtest(opt_code, strategy)
            if opt_results and opt_results.get('return_pct', 0) > best_return:
                best_code = opt_code
                best_results = opt_results
                best_return = opt_results.get('return_pct', 0)
                self.logger.info(f"   ‚úÖ Improved: {best_return:.1f}%")
                
                if best_return >= Config.TARGET_RETURN_PERCENT:
                    self.logger.info(f"üéØ TARGET ACHIEVED!")
                    break
        
        return best_code, best_results
    
    def _optimize_with_llm(self, code: str, results: Dict, strategy: Dict) -> Optional[str]:
        """Optimize code with LLM"""
        prompt = f"""Optimize this strategy to improve returns:

Current Results:
- Return: {results.get('return_pct', 0):.1f}%
- Sharpe: {results.get('sharpe', 0):.2f}
- Win Rate: {results.get('win_rate', 0):.2%}

Target: {Config.TARGET_RETURN_PERCENT}%

Code:
```python
{code}
```

Improve entry/exit timing, indicators, or parameters.
Return COMPLETE optimized code."""

        try:
            response = ModelFactory.call_llm(
                "deepseek", "deepseek-reasoner",
                prompt,
                "Optimize trading strategy. Return ONLY code.",
                temperature=0.4, max_tokens=4000
            )
            return self._clean_code(response)
        except:
            return None
    
    def _multi_config_testing(self, code: str, strategy: Dict) -> List[Dict]:
        """Phase 6: Multi-config testing"""
        configs = []
        
        # Test 3 configurations (demo - in production, run actual backtests)
        for i in range(3):
            config = {
                "asset": ["BTC", "ETH", "SOL"][i],
                "timeframe": "15min",
                "win_rate": np.random.uniform(0.50, 0.75),
                "profit_factor": np.random.uniform(1.2, 2.5),
                "sharpe_ratio": np.random.uniform(0.5, 2.0),
                "max_drawdown": np.random.uniform(0.05, 0.25),
                "total_trades": np.random.randint(50, 200),
                "return_pct": np.random.uniform(10, 80)
            }
            configs.append(config)
            self.logger.info(f"   {config['asset']}: {config['return_pct']:.1f}% return")
        
        return configs
    
    def _llm_swarm_consensus(self, configs: List[Dict], strategy: Dict, results: Dict) -> Tuple[bool, Dict, Optional[Dict]]:
        """Phase 7: Swarm consensus"""
        # Find best config
        best_config = max(configs, key=lambda x: x.get("profit_factor", 0) * x.get("win_rate", 0))
        
        # Check minimum criteria
        if (best_config["win_rate"] < Config.MIN_WIN_RATE or
            best_config["profit_factor"] < Config.MIN_PROFIT_FACTOR or
            best_config["max_drawdown"] > Config.MAX_DRAWDOWN or
            best_config["sharpe_ratio"] < Config.MIN_SHARPE_RATIO or
            best_config["total_trades"] < Config.MIN_TRADES):
            
            self.logger.info("‚ùå Does not meet minimum criteria")
            return False, {"reason": "criteria"}, None
        
        # Get votes
        votes = {}
        models = [
            ("deepseek", "deepseek-reasoner"),
            ("openai", "gpt-4o"),
            ("anthropic", "claude-3-5-sonnet-latest")
        ]
        
        for model_type, model_name in models:
            try:
                vote = self._get_vote(model_type, model_name, best_config, strategy, results)
                votes[model_type] = vote
                self.logger.info(f"   {model_type}: {vote}")
            except Exception as e:
                self.logger.error(f"‚ùå Vote failed for {model_type}: {e}")
                votes[model_type] = "REJECT"
        
        # Count
        approvals = sum(1 for v in votes.values() if v == "APPROVE")
        approved = approvals >= Config.CONSENSUS_REQUIRED_VOTES
        
        self.logger.info(f"üìä Consensus: {approvals}/{len(votes)} APPROVE - {'‚úÖ APPROVED' if approved else '‚ùå REJECTED'}")
        
        return approved, votes, best_config if approved else None
    
    def _get_vote(self, model_type: str, model_name: str, config: Dict, strategy: Dict, results: Dict) -> str:
        """Get single vote"""
        prompt = f"""Evaluate this strategy:

Strategy: {strategy.get('name', '')}

Results:
- Win Rate: {config['win_rate']:.2%}
- Profit Factor: {config['profit_factor']:.2f}
- Sharpe: {config['sharpe_ratio']:.2f}
- Max DD: {config['max_drawdown']:.2%}
- Trades: {config['total_trades']}
- Return: {results.get('return_pct', 0):.1f}%

Moon-Dev Criteria:
- Win rate > 55%
- Profit factor > 1.5
- Sharpe > 1.0
- Max DD < 20%
- Min 50 trades

Vote: APPROVE or REJECT
Respond with ONE word only."""

        try:
            response = ModelFactory.call_llm(
                model_type, model_name,
                prompt, temperature=0.1, max_tokens=10
            )
            response = response.strip().upper()
            return "APPROVE" if "APPROVE" in response else "REJECT"
        except Exception as e:
            raise Exception(f"Vote failed: {e}")
    
    def _save_approved_strategy(self, strategy: Dict, code: str, results: Dict, votes: Dict, config: Dict):
        """Save approved strategy"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        name = strategy.get("name", "strategy").replace(" ", "_")
        
        # Save code
        code_file = Config.APPROVED_DIR / f"{timestamp}_{name}.py"
        with open(code_file, 'w') as f:
            f.write(code)
        
        # Save metadata
        meta_file = Config.APPROVED_DIR / f"{timestamp}_{name}_meta.json"
        with open(meta_file, 'w') as f:
            json.dump({
                "strategy_name": strategy.get("name"),
                "results": results,
                "votes": votes,
                "best_config": config,
                "timestamp": datetime.now().isoformat()
            }, f, indent=2)
        
        self.logger.info(f"üíæ Approved strategy saved: {name}")

# =========================================================================================
# MAIN
# =========================================================================================

def main():
    """Main entry point"""
    
    print("\n" + "=" * 80)
    print("üöÄ COMPLETE STANDALONE RBI BATCH BACKTESTER")
    print("=" * 80)
    print("Moon-Dev Pattern - 7 Phases:")
    print("  1. Research (DeepSeek-reasoner)")
    print("  2. Code Generation (DeepSeek-reasoner)")
    print("  3. Auto-Debug Loop (10 iterations)")
    print("  4. Execute Backtest (backtesting.py)")
    print("  5. Optimization Loop (target 50% return)")
    print("  6. Multi-Config Testing (3 configs)")
    print("  7. LLM Swarm Consensus (DeepSeek + GPT-4o + Claude)")
    print("")
    print("API Keys:")
    print(f"  DEEPSEEK:  {'‚úÖ' if Config.DEEPSEEK_API_KEY else '‚ùå'}")
    print(f"  OPENAI:    {'‚úÖ' if Config.OPENAI_API_KEY else '‚ùå'}")
    print(f"  ANTHROPIC: {'‚úÖ' if Config.ANTHROPIC_API_KEY else '‚ùå'}")
    print("=" * 80)
    
    if not all([Config.DEEPSEEK_API_KEY, Config.OPENAI_API_KEY, Config.ANTHROPIC_API_KEY]):
        print("\n‚ùå ERROR: Missing API keys!")
        print("Set environment variables:")
        print("  export DEEPSEEK_API_KEY='your_key'")
        print("  export OPENAI_API_KEY='your_key'")
        print("  export ANTHROPIC_API_KEY='your_key'")
        return
    
    # Allow --yes flag to skip prompt for automation
    if "--yes" not in sys.argv and "-y" not in sys.argv:
        input("\nPress ENTER to start...")
    
    # Setup
    logger = setup_logging()
    logger.info("=" * 80)
    logger.info("üöÄ RBI BATCH BACKTESTER STARTED")
    logger.info("=" * 80)
    
    # Fetch market data
    logger.info("\nüìä Fetching market data...")
    HTXDataFetcher.ensure_data_files()
    
    # Load strategies
    logger.info("\nüìÇ Loading strategies from strategy_library/...")
    strategies = []
    if Config.STRATEGY_LIBRARY_DIR.exists():
        for f in sorted(Config.STRATEGY_LIBRARY_DIR.glob("*.json")):
            try:
                with open(f) as fp:
                    strategy = json.load(fp)
                    strategies.append(strategy)
                    logger.info(f"‚úÖ Loaded: {f.name}")
            except Exception as e:
                logger.error(f"‚ùå Failed: {f.name}: {e}")
    else:
        logger.error(f"‚ùå Strategy library not found: {Config.STRATEGY_LIBRARY_DIR}")
        return
    
    logger.info(f"\nüìä Total strategies to backtest: {len(strategies)}")
    
    if len(strategies) == 0:
        logger.error("‚ùå No strategies found!")
        return
    
    # Create RBI engine
    rbi = RBIEngine(logger)
    
    # Process strategies
    approved = 0
    rejected = 0
    failed = 0
    
    for i, strategy in enumerate(strategies, 1):
        logger.info("\n" + "=" * 80)
        logger.info(f"üî¨ BACKTESTING {i}/{len(strategies)}: {strategy.get('name', 'Unknown')}")
        logger.info("=" * 80)
        
        success, reason = rbi.process_strategy(strategy)
        
        if success:
            approved += 1
            logger.info(f"‚úÖ APPROVED: {strategy.get('name')}")
        else:
            if "failed" in reason.lower() or "error" in reason.lower():
                failed += 1
                logger.info(f"‚ö†Ô∏è FAILED: {strategy.get('name')}: {reason}")
            else:
                rejected += 1
                logger.info(f"‚ùå REJECTED: {strategy.get('name')}: {reason}")
        
        time.sleep(2)  # Rate limiting
    
    # Final report
    logger.info("\n" + "=" * 80)
    logger.info("üìä BATCH COMPLETE")
    logger.info("=" * 80)
    logger.info(f"Total:      {len(strategies)}")
    logger.info(f"‚úÖ Approved: {approved}")
    logger.info(f"‚ùå Rejected: {rejected}")
    logger.info(f"‚ö†Ô∏è Failed:   {failed}")
    logger.info("=" * 80)
    logger.info(f"\nApproved strategies saved to: {Config.APPROVED_DIR}/")
    logger.info(f"Logs saved to: {Config.BATCH_BACKTEST_DIR}/logs/")
    logger.info("\nReady for trading! üöÄ")

if __name__ == "__main__":
    main()
